---
title: "Exam_MSBA"
author: "Ryan Hoff"
date: "July 20, 2018"
output:
  html_document: default
  pdf_document: default
---

##Problem 2.10

####---A---
```{r}
rm(list=ls())
library(MASS)
#?Boston
cat("The Number of Rows:")
print(nrow(Boston))
cat("The Number of Columns:")
print(ncol(Boston))
```
The rows are different townships within boston and each column is information gathered about the township i.e crime-rate, access to river or highways, median value of homes, etc
There are 506 rows and 14 columns

####---B & C---
```{r}
pairs(Boston[,1:14])
```

This is sort of an information overload but some information can be gathered with a glance.From this you can see a slight positive correlation in crime rate with nox (The nitrogen oxide concentrarion), age (proportion of owner-occupied units built prior to 1940), and possible lstat (lower status of the population). There is also a negative correlation in dis(Weighted mean distance to five Boston employment centres) and low negative correlations in both rm(average number of rooms per dwelling) and medv (median value of owner-occupied homes)

####---D---
```{r}
summary(Boston)
```
Looking at the summary of the data and more specifically large deviations from the mean. We see in the crime rate per capita column, that there is township with a massive per capita crive rate of 88.9 which is nearly 25 times the mean. Some other things that stood out to me was that from age you can see that there has been a recently developed township from the min and there appears to be a township that is strictly on huge tracts of land (100%).

####---E---
```{r}
table(Boston[,4])
```
From the table sumamry of the chas column we can see that 35 townships border the Charles river.

####---F---
```{r}
median(Boston[,11])
```
From the median function we find that the median for pupil-teacher ration by town is 19.05.

####---G---
```{r}
min(Boston[,14])
Boston[which.min(Boston$medv),]
```
From the min function we find there is a township with a $5000 median home value. This value is far from the median of median home values. Glancing at the other values for the township you find some other factors that would be associated with lower-income neighborhoods such as high percentage of low-income residents and high crime rate. 

####---H---
```{r}
sum(Boston[,6]>=7)
sum(Boston[,6]>=8)
```
From the sum function we see that there are only 13 townships with an average number of rooms per dwelling greater or equal to 8 rooms. 

##Problem 3.15

####---A---
```{r}
Boston$chas <- factor(Boston$chas, labels=c("N","Y"))
attach(Boston) #I see this as being very helpful in the future
               # (7/24 Update) You've made me have second thoughts on this function
               # (7/26 Update) This function was causing an error saying data was "masked"
summary(lm_zn <- lm(crim~zn))
#The high t-value indicates siginifigance from large proportions of resident lots over 25k sq ft.
summary(lm_indus <- lm(crim~indus))
#The high t-value indicates signifigance from the strong presence of Industry.
summary(lm_chas <- lm(crim~chas))
#The low t-value indicates no signifigance for bounding the Charles River.
summary(lm_nox <- lm(crim~nox))
#The high t-value indicates signifigance from high notrogen oxide concentrations. 
summary(lm_rm <- lm(crim~rm))
#The high t-value indicates signifigance from the average rooms per dwelling.
summary(lm_age <- lm(crim~rm))
#The high t-value indicates signifigance from the proportion of older owner-occupied homes.
summary(lm_dis <- lm(crim~dis))
#The high t-value indicates signifigance from the weighted mean of distances to Boston employment centres.
summary(lm_rad <- lm(crim~rad))
#The high t-value indicates signifigance from the access to radial highways.
summary(lm_tax <- lm(crim~tax))
#The high t-value indicates signifigance from the full-value porperty-tax rate.
summary(lm_ptratio <- lm(crim~ptratio))
#The high t-value indicates signifigance from the pupil-teacher ratio.
summary(lm_black <- lm(crim~black))
#The high t-value indicates signifigance from the proportion of black residents.
summary(lm_lstat <- lm(crim~lstat))
#The high t-value indicates signifigance from the proportion of lower status population.
summary(lm_medv <- lm(crim~medv))
#The high t-value indicates signifigance from the median value of homes.(YES)
```
####---B---
```{r}
summary(lm_bost <- lm(crim~.,data=Boston))
```
From the resulting summary we can see that zn, dis, rad, black and medv have comparitively lower p-values and therefore reject the null hypothesis.

####---C---
```{r}
Univariate <- c(coefficients(lm_zn)[2],coefficients(lm_indus)[2],coefficients(lm_chas)[2],coefficients(lm_nox)[2],coefficients(lm_rm)[2],coefficients(lm_age)[2],
                coefficients(lm_dis)[2],coefficients(lm_rad)[2],coefficients(lm_tax)[2],coefficients(lm_ptratio)[2],coefficients(lm_black)[2],coefficients(lm_lstat)[2],
                coefficients(lm_medv)[2]) ######Is there a better way to do this?##### #Answer: Yeah I figure you could loop this but couldnt get it working in time.
multi_regression <- coefficients(lm_bost)[2:14]
plot(Univariate, multi_regression, main="Coefs Plot")
```

####---D---
```{r}
summary(lm_zn <- lm(crim~poly(zn,3)))
#Yes, for B1 and B2
summary(lm_indus <- lm(crim~poly(indus,3)))
#Yes, for B1, B2 and B3
#From the previos summary we know 'chas' only has an intercept b0
summary(lm_nox <- lm(crim~poly(nox,3)))
#Yes, for B1, B2 and B3
summary(lm_rm <- lm(crim~poly(rm,3)))
#Yes, for B1 and B2
summary(lm_age <- lm(crim~poly(age,3)))
#Yes, for B1, B2 and B3
summary(lm_dis <- lm(crim~poly(dis,3)))
#Yes, for B1, B2 and B3
summary(lm_rad <- lm(crim~poly(rad,3)))
#Yes, for B1 and B2
summary(lm_tax <- lm(crim~poly(tax,3)))
#Yes, for B1 and B2
summary(lm_ptratio <- lm(crim~poly(ptratio,3)))
#Yes, for B1, B2 and B3
summary(lm_black <- lm(crim~poly(black,3)))
#No, only B0 and B1 so linear
summary(lm_lstat <- lm(crim~poly(lstat,3)))
#Yes, for B1 and B2
summary(lm_medv <- lm(crim~poly(medv,3)))
#Yes, for B1, B2 and B3
detach(Boston)
```

##Problem 6.9

####---A---
```{r}
rm(list=ls())
library(ISLR)
library(MASS)
attach(College)
set.seed(1993)

half <- nrow(College)/2
half_train <- sample(1:nrow(College), half)
train <- College[half_train,]
test <- College[-half_train,]
```
####---B---
```{r}
college_lm <- lm(Apps~., data=train)
college_lm_pred <- predict(college_lm, test)
cat("The test error for lm is ")
print(college_lm_err <- mean((test$Apps - college_lm_pred)^2))
```
####---C---
```{r}
library(glmnet)

train_glm <- model.matrix(Apps~., data=train)[,-1]
test_glm <- model.matrix(Apps~., data=test)[,-1]
college_ridge <- cv.glmnet(train_glm, train$Apps, alpha=0)
cat("The lambda for ridge is ")
print(lambda_r <- college_ridge$lambda.min)
#432.3569 what? This seems super high but this is the Lambda for ridge

college_ridge_pred <- predict(college_ridge, s=lambda_r, newx = test_glm)
cat("The test error for ridge is")
print(college_ridge_err <- mean((test$Apps - college_ridge_pred)^2))
cat("The coefs for ridge are")
print(college_ridge_coef <- predict(college_ridge, type="coefficients",s=lambda_r))
```
####---D---
```{r}
college_lasso <- cv.glmnet(train_glm, train$Apps, alpha=1)
cat("The lambda for lasso is")
print(lambda_l <- college_lasso$lambda.min)

college_lasso_pred <- predict(college_lasso, s=lambda_l, newx=test_glm)
cat("The test error for lasso is")
print(college_lasso_err <- mean((test$Apps - college_lasso_pred)^2))
cat("The coefs for lasso are")
print(college_lasso_coef <- predict(college_lasso, type="coefficients",s=lambda_l))
```
####---E---
```{r}
library(pls)
college_pcr <- pcr(Apps~., data=train, scale=T, validation="CV")
validationplot(college_pcr, val.type = "MSEP")
summary(college_pcr)

college_pcr_pred <- predict(college_pcr, test, ncomp = 17)
cat("The test error for PCR is")
print(college_pcr_err <- mean((test$Apps - college_pcr_pred)^2))
```
####---F---
```{r}
college_pls <- plsr(Apps~., data=train, scale=T, validation="CV")
validationplot(college_pls, val.type = "MSEP")
summary(college_pls)

college_pls_pred <- predict(college_pls, test, ncomp = 12)
cat("The test error for PLS is")
print(college_pls_err <- mean((test$Apps-college_pls_pred)^2))
```
####---G---

Looking at the MSE test errors, the best performances were from ridge and lasso which were barely better than the linear model. PCR and PLS were the worst of the models. 

```{r}
barplot(c(college_lm_err,college_ridge_err,college_lasso_err,college_pcr_err,college_pls_err),
        names.arg = c('lm','ridge','lasso','pcr','pls'), ylab ="Test Error")
detach(College)
```
##Problem 6.11

####---A & B---
```{r}
library(ISLR)
library(MASS)

n = dim(Boston)[1]
set.seed(1993)

half2 <- nrow(Boston)/2
half_train2 <- sample(1:nrow(Boston), half2)
Boston_train <- Boston[half_train2,]
Boston_test <- Boston[-half_train2,]

library(glmnet)
######-----Ridge-----######
train_glm <- model.matrix(crim~., data=Boston_train)[,-1]
test_glm <- model.matrix(crim~., data=Boston_test)[,-1]
Boston_ridge <- cv.glmnet(train_glm, Boston_train$crim, alpha=0)
cat("The lambda for ridge is ")
print(lambda_r <- Boston_ridge$lambda.min)

Boston_ridge_pred <- predict(Boston_ridge, s=lambda_r, newx = test_glm)
cat("The test error for ridge is")#55.255
print(Boston_ridge_err <- mean((Boston_test$crim - Boston_ridge_pred)^2))
#cat("The coefs for ridge are")
#print(Boston_ridge_coef <- predict(Boston_ridge, type="coefficients",s=lambda_r))
```

```{r}
######-----Lasso-----######
Boston_lasso <- cv.glmnet(train_glm, Boston_train$crim, alpha=1)
cat("The lambda for lasso is")
print(lambda_l <- Boston_lasso$lambda.min)

Boston_lasso_pred <- predict(Boston_lasso, s=lambda_l, newx=test_glm)
cat("The test error for lasso is")#54.664
print(Boston_lasso_err <- mean((Boston_test$crim - Boston_lasso_pred)^2))
cat("The coefs for lasso are")
print(Boston_lasso_coef <- predict(Boston_lasso, type="coefficients",s=lambda_l))
```
```{r}
######-----PCR-----######
library(pls)
Boston_pcr <- pcr(crim~., data=Boston_train, scale=T, validation="CV")
validationplot(Boston_pcr, val.type = "MSEP")
summary(Boston_pcr)

Boston_pcr_pred <- predict(Boston_pcr, Boston_test, ncomp = 12)
cat("The test error for PCR is") #55.8
print(Boston_pcr_err <- mean((Boston_test$crim - Boston_pcr_pred)^2))
```
```{r}
######-----PLS-----######
Boston_pls <- plsr(crim~., data=Boston_train, scale=T, validation="CV")
validationplot(Boston_pls, val.type = "MSEP")
summary(Boston_pls)

Boston_pls_pred <- predict(Boston_pls, Boston_test, ncomp = 6)
cat("The test error for PLS is") #54.67
print(college_pls_err <- mean((Boston_test$crim-Boston_pls_pred)^2))
```
####---C---

Based on the test errors from the cross validated ridge, lasso, pcr and pls models lasso appears to be the best but by a slim margin. The lasso model also takes into account all the variables in Boston but only certain variables bear signifigant weight to the model.
These variables include chas, nox, dis, rad, ptratio. 

##Problem 4.10 

####---A---
```{r}
rm(list=ls())
library(ISLR)
library(kknn)

summary(Weekly)
pairs(Weekly)
```
The only correlation appears to be a positive correlation between Year and Volume.

####---B---
```{r}
summary(logifit <- glm(Direction~.,data=Weekly[,c(-1,-8)], family=binomial))
```
Lag2 appears to be the only signifigant predictor

####---C---
```{r}
logiprob <- predict(logifit, Weekly, type="response")
logipred <- ifelse(logiprob > 0.5, "Up", "Down")
table(logipred, Weekly$Direction)
print(logi_f <- mean(logipred == Weekly$Direction))
```
The table is telling us that when the prediction is down the model is right about 53% of the time (54/(54+48)) and that when the precition is up the model is right about 43% percent of the time (430/430+557).The overall fraction of right predictions is 0.561

####---D---
```{r}
Years <- Weekly$Year %in% (1990:2008)

train <- Weekly[Years,]
test <- Weekly[!Years,]

Year_fit <- glm(Direction~Lag2, data=train, family=binomial)
Year_prob <- predict(Year_fit, test, type="response")
Year_pred <- ifelse(Year_prob > 0.5, "Up", "Down")
table(Year_pred, test$Direction)
cat("The overall fraction of right predictions is")
print(Year_fit_f <- mean(Year_pred == test$Direction))
```
####---G---
```{r}
require(class)
set.seed(1993)

train2 <- as.matrix(train$Lag2)
test2 <- as.matrix(test$Lag2)
train_dir <- train$Direction

Year_knn <- knn(train2, test2, train_dir, k=1)
table(Year_knn, test$Direction)
cat("The overall fraction of right predictions is")
print(Year_knn_f <- mean(Year_knn == test$Direction))
```
####---H---

The logistic regression with training gave the best overall fraction of right predictions.

####---I---

First, Im gonna try a few more K values for KNN, 5, 10, 25
Second, Lets see how logistic regression with training performs with multiple predictors Lag2:Lag1 & Lag2:Lag4
```{r}
Year_knn5 <- knn(train2, test2, train_dir, k=5)
table(Year_knn5, test$Direction)
cat("The overall fraction of right predictions for k=5 is")
print(Year_knn5_f <- mean(Year_knn5 == test$Direction)) #0.538


Year_knn10 <- knn(train2, test2, train_dir, k=10)
table(Year_knn10, test$Direction)
cat("The overall fraction of right predictions for k=10 is")
print(Year_knn10_f <- mean(Year_knn10 == test$Direction)) #0.567


Year_knn25 <- knn(train2, test2, train_dir, k=25)
table(Year_knn25, test$Direction)
cat("The overall fraction of right predictions for k=25 is")
print(Year_knn25_f <- mean(Year_knn25 == test$Direction)) #0.528


Year_fit21 <- glm(Direction~Lag2:Lag1, data=train, family=binomial)
Year_prob21 <- predict(Year_fit21, test, type="response")
Year_pred21 <- ifelse(Year_prob21 > 0.5, "Up", "Down")
table(Year_pred21, test$Direction)
cat("The overall fraction of right predictions for Lag2:Lag1 is")
print(Year_fit21_f <- mean(Year_pred21 == test$Direction)) #0.586


Year_fit24 <- glm(Direction~Lag2:Lag4, data=train, family=binomial)
Year_prob24 <- predict(Year_fit24, test, type="response")
Year_pred24 <- ifelse(Year_prob24 > 0.5, "Up", "Down")
table(Year_pred24, test$Direction)
cat("The overall fraction of right predictions for Lag2:Lag4 is")
print(Year_fit24_f <-mean(Year_pred24 == test$Direction)) #0.5576
```

For my experimentation fitting a logistic regression with Lag2:Lag1 gave the best overall fraction fo right predictions.

##Problem 8.8

####---A---
```{r}
rm(list=ls())
library(ISLR)
attach(Carseats)
data <- dim(Carseats)[1]
set.seed(1993)

#View(Carseats)
train <- sample(data[1], (data[1]/2))
training_seat <- Carseats[train,]
testing_seat <- Carseats[-train,]
#View(training_seat)
```
####---B---
```{r}
library(tree)
Carseats_tree <- tree(Sales~., data=training_seat)
plot(Carseats_tree)
text(Carseats_tree)

Carseats_tree_pred <- predict(Carseats_tree, testing_seat)
cat('The tree MSE:')
print(Carseats_tree_f <- mean((testing_seat$Sales-Carseats_tree_pred)^2))
```
####---C---
```{r}
#####C#####
Carseats_cv <- cv.tree(Carseats_tree, FUN = prune.tree)

plot(Carseats_cv$size, Carseats_cv$dev, type = "l")
print("From this graph we see that the optimal complexity is at the highest number of trees, 16")
Carseats_tree_prune <- prune.tree(Carseats_tree, best = 16)
plot(Carseats_tree_prune)
text(Carseats_tree_prune, pretty = 0)

Prune_pred <- predict(Carseats_tree_prune, testing_seat)
cat('The pruned tree MSE:')
print(Carseats_tree_f <- mean((testing_seat$Sales - Prune_pred)^2))
```
####---D---
```{r}
library(randomForest)
#View(Carseats)
Air_bag <- randomForest(Sales~., data = training_seat, ntree = 100, mtry = 10, importance = T)
Air_bag_pred <- predict(Air_bag, testing_seat)
cat('The bagging MSE:')
print(Air_bag_f <- mean((testing_seat$Sales - Air_bag_pred)^2)) 
#varImpPlot(Air_bag) #I find the graph to be the easier way to determine m
importance(Air_bag)
```
####---E---
```{r}
Air_random <- randomForest(Sales~., data = training_seat, ntree = 100, importance = T)
Air_random_pred <- predict(Air_random, testing_seat)
cat('The random forest MSE:')
print(Air_random_f <- mean((testing_seat$Sales - Air_random_pred)^2))
#varImpPlot(Air_random) # From this graph and importance() it looks like 5 would be a good value for mtry
importance(Air_random)

#Setting to 5
Air_random_5 <- randomForest(Sales~., data = training_seat, ntree = 100, mtry = 5, importance = T)
Air_random_pred5 <- predict(Air_random_5, testing_seat)
cat('The random forest MSE:')
print(Air_random_f5 <- mean((testing_seat$Sales - Air_random_pred5)^2)) #Siginifigant lowering of MSE
detach(Carseats)
```

Adjusting the mtry value towards the number of important variables (about 5 from the importance() data) lowers the MSE. The MSE is .5 lower (2.86) with mtry =5  than default (3.35).

##Problem 8.11

####---A---
```{r}
library(ISLR)
attach(Caravan)
#detach(Caravan)
set.seed(1993)

Caravan$Purchase <- ifelse(Caravan$Purchase == 'Yes',1,0)
Caravan_train <- Caravan[1:1000,]
Caravan_test <- Caravan[-(1:1000),]
```
####---B---
```{r}
library(gbm)
summary(Caravan_boost <- gbm(Purchase~., data = Caravan_train, n.trees = 1000, shrinkage = 0.01, distribution='bernoulli'))
```
The variables that appear to be the most important are, PPERSAUT, MKOOPKLA, MOPLHOOG & MBERMIDD.

####---C---
```{r}
Caravan_boost_prob <-  predict(Caravan_boost, Caravan_test, n.trees = 1000, type ="response")
Caravan_boost_pred <- ifelse(Caravan_boost_prob > 0.2, 1, 0)
table(Caravan_test$Purchase, Caravan_boost_pred)
cat("The precentage of people predicted to make a purchase is")
print(32/(128+32)*100) #About 20% 

#Logistic Model
Caravan_lm <- glm(Purchase~., data = Caravan_train, family=binomial)
Caravan_lm_prod <- predict(Caravan_lm, Caravan_test, type="response")
Caravan_lm_pred <- ifelse(Caravan_lm_prod > 0.2, 1, 0)
table(Caravan_test$Purchase, Caravan_lm_pred)
cat("The precentage of people predicted (from logrithmic regression) to make a purchase is")
print(58/(350+58)*100) #About 14%

#Linear Model
Caravan_l <- lm(Purchase~.,data=Caravan)
Caravan_l_prod <- predict(Caravan_l, Caravan_test, type="response")
Caravan_l_pred <- ifelse(Caravan_l_prod > 0.2, 1, 0)
table(Caravan_test$Purchase, Caravan_l_pred)
cat("The precentage of people predicted (from linear regression) to make a purchase is")
print(27/(61+27)*100) #About 31%
detach(Caravan)
```

The log regression at 14% is worse than the boost model at 21% .The linear model was at 31% making it the best model, prooving the simplest option can be the best option. 

##Problem 1

####---A---
```{r}
rm(list=ls())
library(MASS)
BEAU <- read.csv("https://faculty.mccombs.utexas.edu/carlos.carvalho/teaching/BeautyData.csv")
attach(BEAU)
plot(CourseEvals~BeautyScore)#Lets just see if there is any visible trend. There does appear to be a positive correlation between beauty and course evaluations. So Im going to start by fitting a linear model and then complicate it from there. 
BEAU_lm <- lm(CourseEvals~BeautyScore)
plot(CourseEvals~BeautyScore, main="General Linear Model Fit")
abline(BEAU_lm$coef[1],BEAU_lm$coef[2],col=4,lwd=1) #The Linear Model seems to do fine, now lets see the effects of the other variables
detach(BEAU)

#Men v. Women?
BEAU_men <- BEAU[which(BEAU$female == "0"), ]#Only Boys Allowed
BEAU_men_lm <- lm(BEAU_men$CourseEvals~BEAU_men$BeautyScore)
BEAU_women <- BEAU[-which(BEAU$female == "0"),] #Only Girls Allowed
BEAU_women_lm <- lm(BEAU_women$CourseEvals~BEAU_women$BeautyScore) 

plot(BEAU$CourseEvals~BEAU$BeautyScore, main="Linear Model Fit (Men v. Women)")
abline(BEAU_lm$coef[1],BEAU_lm$coef[2],col=1,lwd=1)
abline(BEAU_men_lm$coef[1],BEAU_men_lm$coef[2],col="dodgerblue3",lwd=2)
abline(BEAU_women_lm$coef[1],BEAU_women_lm$coef[2],col="lightpink3",lwd=2)
```

This reveals some interesting trends. On average men are rated better than women, but the trend for female teachers to be rater higher if they are seen as more attractive is stronger than for male teachers.
```{r}
#Lower Division Class?
BEAU_low <- BEAU[which(BEAU$lower == "1"),] 
BEAU_low_lm <- lm(BEAU_low$CourseEvals~BEAU_low$BeautyScore) 

#Native Language Not English?
BEAU_non <- BEAU[which(BEAU$nonenglish == "1"),] 
BEAU_non_lm <- lm(BEAU_non$CourseEvals~BEAU_non$BeautyScore) 

#Tenured?
BEAU_ten <- BEAU[which(BEAU$tenuretrack == "1"),] 
BEAU_ten_lm <- lm(BEAU_ten$CourseEvals~BEAU_ten$BeautyScore) 

plot(BEAU$CourseEvals~BEAU$BeautyScore, main="Linear Model Fit (Lower Class v Native Language v Tenure Track)")
abline(BEAU_lm$coef[1],BEAU_lm$coef[2],col=1,lwd=1)
abline(BEAU_low_lm$coef[1],BEAU_low_lm$coef[2],col="lightsalmon2",lwd=2)
abline(BEAU_non_lm$coef[1],BEAU_non_lm$coef[2],col="seagreen3",lwd=2)
abline(BEAU_ten_lm$coef[1],BEAU_ten_lm$coef[2],col="orange3",lwd=2)
```

####---B---

For the other possible determinants besides sex, the only one that has a strong correlation is nonenglish. According to the linear model, if your native language isnt english you can make up for it to an extreme by being attractive. Now this just could be because there are signifigantly less data points for nonenglish speakers but still the trend is strong. Being lower class seems to have almsot no effect on course ratings and being tenured, rather hilariously, gives lower ratings but with an almost identical slope to the general linear fit.  
Now, to summarize and address what the professor was quoted on. The effects appear signifigant in the models, but the models are only there to give us hints not answers. To make that "probably impossible" conclusion we would need a completely whole dataset, every professor, every rating and every student insight, and thats.... probably impossible. We are just left with the hints given to us by our models, in which we can only educated guess if their trend "represents productivity or discrimination".  

##Problem 2

####---A---
```{r}
MidCity <- read.csv('https://faculty.mccombs.utexas.edu/carlos.carvalho/teaching/MidCity.csv')
MidCity_Nbhd3 <- MidCity[MidCity$Nbhd == 3,] #For C

attach(MidCity)
set.seed(1993)

#Is there a premium for brick homes?
summary(brick_lm <- lm(Price~Brick))
```

Yes, there is a premium for brick homes. If the home is made of brick, all other things being equal, the price is raised by $25,811.

####---B---
```{r}
#Is there a premium for homes in neighborhood 3?
summary(Nbhd_lm <- lm(Price~as.factor(Nbhd)))
```

Yes, there is a premium for homes in neighborhood 3. A home is neighborhood 3, on average, costs $49,110 more than a home in neighborhood 1.

####---C---
```{r}
summary(Nbhd3Brick_lm <- lm(Price~Brick,data=MidCity_Nbhd3))
```
Yes, there is a premium for brick homes in neighborhood 3. A brick home in neighborhood 3, on average, costs $26,970 more than a non-brick home in neighborhood 3.

####---D---
```{r}
MidCity_copy <- MidCity
MidCity_copy$Nbhd <- replace(MidCity_copy$Nbhd,which(MidCity_copy$Nbhd == 2),1)
summary(Nbhd2_lm <- lm(Price~as.factor(Nbhd),MidCity_copy))
```

Yes, you can combine neighborhood 1 and 2. The r squared falls from 0.565 to 0.5098 so there's an improvement but it comes at a cost of more generalization. Considering that there are already only 3 neighborhood seperations, I would not combine the first two as their b1 and b0 values vary...but I guess you can. 

##Problem 3

####---A---

The data behind why a city hires more cops is not based solely on the amount of crime. The reseacrhers found out the data is far more messy/abstract and can have far more predictors for both the amount of cops and the amount of crime.

####---B---

An example for this data "messiness" was in Washington, D.C. The amount of cops in a given public location in D.C is based around the terrorism alert system, not just street crime. On the 'High-Alert' days they saw a clear reduction of crime as seen in table 2 but they weren't certain that this was just due to additional police in the streets.

####---C---

As the interviewer(?) pointed out, the criminals could just be ,"...hiding in their rooms because they're afraid of the elevated terror level." There was a reduction of daily crime on these 'High-Alert' days, but was it actually from the extra police? The researchers used METRO ridership data to prove there was not just a decline of "victims in the street" due to the elevated terror level. Since the METRO ridership wasn't diminished on 'High-Alert' days, they felt the active population was unchanged. 

####---D---

The model is correlating the amount of crime incidents on a given day to the level of police presence, while referencing the METRO ridership data to ensure that there is a comparible active population each day. The model indicates that there are consistently less crime incidents on 'High-Alert' days (-11.058 crimes), less (-2.621 crimes) for disrtict 1 and slightly less (-0.571 crimes) for all other districts. 


##Problem 4
```{r}
###---This is cal_setup.txt---###
ca <- read.csv('https://faculty.mccombs.utexas.edu/carlos.carvalho/teaching/CAhousing.csv')
ca$AveBedrms <- ca$totalBedrooms/ca$households
ca$AveRooms <- ca$totalRooms/ca$households
ca$AveOccupancy <- ca$population/ca$households
logMedVal <- log(ca$medianHouseValue)
ca <- ca[,-c(4,5,9)] # lose lmedval and the room totals
ca$logMedVal = logMedVal

set.seed(99)
n=nrow(ca)
n1=floor(n/2)
n2=floor(n/4)
n3=n-n1-n2
ii = sample(1:n,n)
catrain=ca[ii[1:n1],]
caval = ca[ii[n1+1:n2],]
catest = ca[ii[n1+n2+1:n3],]
```

```{r}
library(BART)
library(randomForest)
library(gbm)
#Im just going to use the values used in the example 
#just because as of right now Im not 100% clear on Bart
nd = 200 
burn = 50 
x.bart <- catrain[,-10]
y.bart <- catrain[,10]
ca_bart <- wbart(x.bart, y.bart, nskip=burn, ndpost=nd)
ca_bart_pred = predict(ca_bart, as.matrix(catest[,-10]))
ca_bart_prob = apply(ca_bart_pred, 2, mean)
cat("The RMSE of Bart is...")
print(sqrt(mean((ca_bart_pred - catest[,10])^2))) #0.7686838

ca_rf <- randomForest(logMedVal~., data=catrain, mtry=5)
ca_rf_pred <- predict(ca_rf, newdata = catest)
cat("The RMSE of random forest is...")
print(sqrt(mean((ca_rf_pred-catest[,"logMedVal"])^2))) #0.2356248

ca_boost <- gbm(logMedVal~., data=catrain,distribution= "gaussian", n.trees=500,
                interaction.depth=4, shrinkage=0.1, verbose =T)
ca_boost_pred = predict(ca_boost, catest, n.trees = 500)
cat("The RMSE of boost is...")
print(sqrt(mean((ca_boost_pred-catest[,"logMedVal"])^2))) #0.2373844
```

The Bart results are terrible, 3 times worse than both randomforest and gbm. Im guessing this is due to a lack of optimisation, but the error is so much higher that randomforest and gbm are gaurenteed to be better.

##Problem 5
```{r}
library(nnet)
library(MASS)
attach(Boston)
set.seed(1993)
#Again being guided by the example code
train <- sample(1:nrow(Boston),nrow(Boston)/2)
Boston_train = Boston[train,]
Boston_test=Boston[-train,]

Boston_NN1 <- nnet(medv~.,Boston_train,size=3,decay=.5,linout=T)
Boston_NN2 <- nnet(medv~.,Boston_train,size=3,decay=.00001,linout=T)
Boston_NN3 <- nnet(medv~.,Boston_train,size=50,decay=.5,linout=T)
Boston_NN4 <- nnet(medv~.,Boston_train,size=50,decay=.00001,linout=T)

Boston_NN1_pred = predict(Boston_NN1,Boston_test)
Boston_NN2_pred = predict(Boston_NN2,Boston_test)
Boston_NN3_pred = predict(Boston_NN3,Boston_test)
Boston_NN4_pred = predict(Boston_NN4,Boston_test)

cat("The RMSE for a Neural Net 1 is")
print(sqrt(mean((Boston_NN1_pred-Boston_test[,"medv"])^2))) #4.899
cat("The RMSE for a Neural Net 2 is")
print(sqrt(mean((Boston_NN2_pred-Boston_test[,"medv"])^2))) #9.203
cat("The RMSE for a Neural Net 3 is")
print(sqrt(mean((Boston_NN3_pred-Boston_test[,"medv"])^2))) #5.583
cat("The RMSE for a Neural Net 4 is")
print(sqrt(mean((Boston_NN4_pred-Boston_test[,"medv"])^2))) #5.302
detach(Boston)
```
The neural net with the lowest size and high decay (Neural Net 1) is the best option for predicting 'medv'.

##Problem 6

My project was a wholesome experience. I'd like to start by extending a thanks to my group. Early on it I felt like I was by far the weakest link in the group, all other members had decent R experience under their belt. Having not seen R before the class, I immedietly started to feel behind.I know, "Don't compare your progress with others in the class..." but that's really difficult to avoid.I expressed this concern to my group and instead of just leaving me in the dust on the project, they made sure I wasnt left behind.They helped to catch me up to speed, gave me direction in the project, always asked if I had questions and offered explanations.In the end, the project turned out really great and I believe I was able to put in some solid work.  
After we cleaned the data, we assigned models to group members and attempted to apply them. I at first was tasked with fitting a knn model to the data, but we didnt end up liking the results from knn so I moved on to random forest. After getting random forest working, I played around with the random forest settings, number of trees, random variables it's trying (mtry), turning proximity on and off and with or without bagging. It was definitely a dance between finding what my beefy computer could actually run (with the data set being 500,000 rows) and what was resonable to run. Ended up trying out 50, 100, 250 trees,with and without bagging, deciding on an mtry of 4 and proximity set to 'false' because I dont have the 1800 GB of ram for the 'true' setting. When I was done with the models, I assisted with the presentation and in finalizing the whole code.
  